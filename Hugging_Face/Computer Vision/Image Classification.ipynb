{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe8fe93e",
   "metadata": {},
   "source": [
    "# Image Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa21d4",
   "metadata": {},
   "source": [
    "**Image classification** is the task of assigning a label or class to an entire image. \n",
    "\n",
    "Images are expected to have only one class for each image.\n",
    "\n",
    "Image classification models take an image as input and return a prediction about which class the image belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39e6cc",
   "metadata": {},
   "source": [
    "### **Use Cases**\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba62549",
   "metadata": {},
   "source": [
    "Image classification models can be used when we are not interested in specific instances of objects with location information or their shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ca3f3",
   "metadata": {},
   "source": [
    "* **Keyword Classification**\n",
    "\n",
    "Image classification models are used widely in stock photography to assign each image a keyword."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273d0a4",
   "metadata": {},
   "source": [
    "* **Image Search**\n",
    "\n",
    "Models trained in image classification can improve user experience by organizing and categorizing photo galleries on the phone or in the cloud, on multiple keywords or tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c3ccfd",
   "metadata": {},
   "source": [
    "### **Inference**\n",
    "\n",
    "---\n",
    "\n",
    "* With the transformers library, you can use the image-classification pipeline to infer with image classification models. \n",
    "* You can initialize the pipeline with a model id from the Hub. \n",
    "* If you do not provide a model id it will initialize with google/vit-base-patch16-224 by default. \n",
    "* When calling the pipeline you just need to specify a path, http link or an image loaded in PIL. \n",
    "* You can also provide a top_k parameter which determines how many results it should return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4b56c",
   "metadata": {},
   "source": [
    "### **Metrics for Image Classification**\n",
    "\n",
    "---\n",
    "\n",
    "**accuracy** : \n",
    "* Accuracy is the proportion of correct predictions among the total number of cases processed. \n",
    "* It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN) \n",
    "* Where: TP: True positive TN: True negative FP: False positive FN: False negative\n",
    "\n",
    "\n",
    "**Recall** : \n",
    "* Recall is the fraction of the positive examples that were correctly labeled by the model as positive. \n",
    "* It can be computed with the equation: Recall = TP / (TP + FN) \n",
    "* Where TP is the true positives and FN is the false negatives.\n",
    "\n",
    "\n",
    "**precision** : \n",
    "* Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. \n",
    "* It is computed via the equation: Precision = TP / (TP + FP) \n",
    "* where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
    "\n",
    "\n",
    "\n",
    "**f1** : \n",
    "* The F1 score is the harmonic mean of the precision and recall. \n",
    "* It can be computed with the equation: F1 = 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e9360",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fc73b",
   "metadata": {},
   "source": [
    "##### Importing Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0305aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cad1a",
   "metadata": {},
   "source": [
    "##### Initilizing Pipeline for Image Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478e2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43a499534d043e9adb663602b7a6809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bda013b2ace477ca1ffed22a7b41091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b372ab5e946a40b285ad8df38c316e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf = pipeline(\"image-classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580a1e6",
   "metadata": {},
   "source": [
    "##### Sample run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3005ccd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.7772114276885986, 'label': 'tabby, tabby cat'},\n",
       " {'score': 0.13951493799686432, 'label': 'Egyptian cat'},\n",
       " {'score': 0.0755685493350029, 'label': 'tiger cat'},\n",
       " {'score': 0.0012304026167839766, 'label': 'Persian cat'},\n",
       " {'score': 0.001177904661744833, 'label': 'lynx, catamount'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"cat.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad25f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9720670580863953, 'label': 'teddy, teddy bear'},\n",
       " {'score': 0.0011989388149231672, 'label': 'toyshop'},\n",
       " {'score': 0.00042086245957762003, 'label': 'wool, woolen, woollen'},\n",
       " {'score': 0.0002505462907720357,\n",
       "  'label': 'backpack, back pack, knapsack, packsack, rucksack, haversack'},\n",
       " {'score': 0.0002280423213960603,\n",
       "  'label': 'indri, indris, Indri indri, Indri brevicaudatus'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"teddy_bear.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba9a98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9483337998390198, 'label': 'shoe shop, shoe-shop, shoe store'},\n",
       " {'score': 0.007263416424393654, 'label': 'cowboy boot'},\n",
       " {'score': 0.0023506777361035347, 'label': 'toyshop'},\n",
       " {'score': 0.001980326371267438, 'label': 'sandal'},\n",
       " {'score': 0.0019447959493845701, 'label': 'clog, geta, patten, sabot'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"girl.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "318d2c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9126858711242676, 'label': 'Granny Smith'},\n",
       " {'score': 0.01985348016023636, 'label': 'orange'},\n",
       " {'score': 0.009205435402691364, 'label': 'strawberry'},\n",
       " {'score': 0.007924770005047321, 'label': 'banana'},\n",
       " {'score': 0.005471340846270323, 'label': 'lemon'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"apple.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5931ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08751760423183441, 'label': 'mashed potato'},\n",
       " {'score': 0.041199881583452225, 'label': 'can opener, tin opener'},\n",
       " {'score': 0.03583018481731415, 'label': 'butternut squash'},\n",
       " {'score': 0.02535988762974739, 'label': 'cucumber, cuke'},\n",
       " {'score': 0.02481667324900627,\n",
       "  'label': 'grocery store, grocery, food market, market'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf(\"potato.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec936f",
   "metadata": {},
   "source": [
    "### **Source :**\n",
    "\n",
    "* https://huggingface.co/tasks/image-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
